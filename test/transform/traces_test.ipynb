{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Generation and Modeling\n",
    "\n",
    "To test our DeepValidate approach we generate a dataset of test traces from a chain of relatively simple arithmetical functions operating on a series of randomized inputs. Given the generated program traces, we train a LSTM classifier to predict whether the output will be valid or result in an error. \n",
    "\n",
    "The trace generation is performed by `output_trace.jl` which reproduces much of the functionality of `varextract.jl` with some important differences. Rather than send trace information to `stdout`, we direct the traces to a file `traces.dat`. This raw output is then processed into a CSV of traces (minus the error dumps we want to predict) and a CSV of binary (0, 1) labels indicating whether the run resulted in an error. \n",
    "\n",
    "(It must be noted that this is not possible within an IJulia notebook due to restrictions on [task switching in staged functions](https://github.com/JuliaLang/julia/issues/18568) which prevents the trace outputs from being written to a file recursively. However, this works just fine from the command line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Cassette.overdub(ctx::TraceCtx,\n",
    "                          f,\n",
    "                          args...)\n",
    "    open(\"traces.dat\", \"a\") do file\n",
    "        write(file, string(f))\n",
    "        write(file, string(args))\n",
    "    end\n",
    "    \n",
    "    # if we are supposed to descend, we call Cassette.recurse\n",
    "    if Cassette.canrecurse(ctx, f, args...)\n",
    "        subtrace = (Any[],Any[])\n",
    "        push!(ctx.metadata[1], (f, args) => subtrace)\n",
    "        newctx = Cassette.similarcontext(ctx, metadata = subtrace)\n",
    "        retval = Cassette.recurse(newctx, f, args...)\n",
    "        # push!(ctx.metadata[2], subtrace[2])\n",
    "    else\n",
    "        retval = Cassette.fallback(ctx, f, args...)\n",
    "        push!(ctx.metadata[1], :t)\n",
    "        push!(ctx.metadata[2], retval)\n",
    "    end\n",
    "    @info \"returning\"\n",
    "    @show retval\n",
    "    return retval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify our `@textset` so that it creates the `traces.dat` file and then loops through a large number of randomized runs of our arithmetic tests. Error conditions happen most often when our inputs are sufficiently close to zero, so a Normal(0,2) distribution gives us a good range of values to generate a reasonable percentage of \"bad\" traces on which to train. Empirically the share of \"bad\" traces generated is about 15-17%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@testset \"TraceExtract\" begin\n",
    "    g(x) = begin\n",
    "        y = add(x.*x, -x)\n",
    "        z = 1\n",
    "        v = y .- z\n",
    "        s = sum(v)\n",
    "        return s\n",
    "    end\n",
    "    h(x) = begin\n",
    "        z = g(x)\n",
    "        zed = sqrt(z)\n",
    "        return zed\n",
    "    end\n",
    "\n",
    "    open(\"traces.dat\", \"w\") do f\n",
    "        write(f, \"\")\n",
    "    end\n",
    "\n",
    "    seeds = rand(Normal(0,2),30000,3)\n",
    "    \n",
    "    for i=1:size(seeds,1)\n",
    "        ctx = TraceCtx(pass=ExtractPass, metadata = (Any[], Any[]))\n",
    "        try\n",
    "            result = Cassette.overdub(ctx, h, seeds[i,:])\n",
    "        catch DomainError\n",
    "            dump(ctx.metadata)\n",
    "        finally\n",
    "            open(\"traces.dat\", \"a\") do f\n",
    "                write(f, \"\\n\")\n",
    "            end\n",
    "        end\n",
    "        if i%1000 == 0\n",
    "            @info string(i)\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating our raw traces, a small amount of pre-processing is required before attempting to model around them. First, we classify our \"good\" and \"bad\" traces based on whether they have resulted in an error. \n",
    "\n",
    "We then need to strip out the actual error dump information from our \"bad\" traces, as this would too easily give away the prediction game. All traces end just before they would error, allowing the validation model to predict the that next outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = split(String(read(\"traces.dat\")), \"\\n\");\n",
    "Ys = Int.(occursin.(Ref(r\"(Base[\\S(?!\\))]+error)\"i), text));\n",
    "\n",
    "text = split.(text, Ref(r\"(Base[\\S(?!\\))]+error)\"i));\n",
    "text = [t[1] for t in text];\n",
    "\n",
    "sum(Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save our traces and our labels our as CSV files for easy ingestion for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedlm( \"traces.csv\",  text[1:end-1], ',')\n",
    "writedlm( \"y_results.csv\",  Ys[1:end-1], ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Classifier Model\n",
    "For our modeling, we use [Flux.jl](https://github.com/FluxML/Flux.jl) and train an LSTM encoder/decoder classifier on our traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General`\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m git-repo `https://github.com/JuliaRegistries/General.git`\n",
      "\u001b[?25l\u001b[2K\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#Pkg.add([\"Flux\", \"MLDataPattern\", \"DelimitedFiles\"])\n",
    "Pkg.activate(\"../../\")\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /home/jfairbanks6/.julia/compiled/v1.0/Flux/QdkVy.ji for Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n",
      "└ @ Base loading.jl:1190\n",
      "┌ Info: Recompiling stale cache file /home/jfairbanks6/.julia/compiled/v1.0/MLDataPattern/qdIQj.ji for MLDataPattern [9920b226-0b2a-5f5f-9153-9aa70a013f8b]\n",
      "└ @ Base loading.jl:1190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "get_data (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "using Flux\n",
    "using Flux: onehot, throttle, crossentropy, onehotbatch, params, shuffle\n",
    "using MLDataPattern: stratifiedobs\n",
    "using Base.Iterators: partition\n",
    "\n",
    "include(\"../../src/validation/utils.jl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"}(*, [-0.34912857])ad6+Bseroctbmilznpy{DfuASqvF<_Mhwx:g\\\"#\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# Set up inputs for model\n",
    "#\n",
    "\n",
    "# Read lines from traces.dat text in to arrays of characters\n",
    "# Convert to onehot matrices\n",
    "\n",
    "cd(@__DIR__)\n",
    "\n",
    "text, alphabet, N = get_data(\"traces.dat\")\n",
    "stop = onehot('\\n', alphabet);\n",
    "prod(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition into subsequences to input to our model\n",
    "\n",
    "seq_len = 50\n",
    "\n",
    "Xs = [collect(partition(t,seq_len)) for t in text];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31-element Array{String,1}:\n",
       " \"getfield(Main, Symbol(\\\"#h#9\\\")){getfield(Main, Symb\"  \n",
       " \"ol(\\\"#g#8\\\"))}(getfield(Main, Symbol(\\\"#g#8\\\"))())([1.\"\n",
       " \"2772, -3.93049, -1.6268],)getfield(getfield(Main, \"    \n",
       " \"Symbol(\\\"#h#9\\\")){getfield(Main, Symbol(\\\"#g#8\\\"))}(ge\"\n",
       " \"tfield(Main, Symbol(\\\"#g#8\\\"))()), :g)getfield(Main,\"  \n",
       " \" Symbol(\\\"#g#8\\\"))()([1.2772, -3.93049, -1.6268],)Ba\"  \n",
       " \"se.Broadcast.broadcasted(*, [1.2772, -3.93049, -1.\"    \n",
       " \"6268], [1.2772, -3.93049, -1.6268])Base.Broadcast.\"    \n",
       " \"materialize(Base.Broadcast.Broadcasted(*, ([1.2772\"    \n",
       " \", -3.93049, -1.6268], [1.2772, -3.93049, -1.6268])\"    \n",
       " \"),)Base.Broadcast.instantiate(Base.Broadcast.Broad\"    \n",
       " \"casted(*, ([1.2772, -3.93049, -1.6268], [1.2772, -\"    \n",
       " \"3.93049, -1.6268])),)copy(Base.Broadcast.Broadcast\"    \n",
       " ⋮                                                       \n",
       " \"2, 4.27327], 1)Base.Broadcast.materialize(Base.Bro\"    \n",
       " \"adcast.Broadcasted(-, ([0.354036, 19.3792, 4.27327\"    \n",
       " \"], 1)),)Base.Broadcast.instantiate(Base.Broadcast.\"    \n",
       " \"Broadcasted(-, ([0.354036, 19.3792, 4.27327], 1)),\"    \n",
       " \")copy(Base.Broadcast.Broadcasted{Base.Broadcast.De\"    \n",
       " \"faultArrayStyle{1}}(-, ([0.354036, 19.3792, 4.2732\"    \n",
       " \"7], 1)),)sum([-0.645964, 18.3792, 3.27327],)sqrt(2\"    \n",
       " \"1.00652295856412,)zero(21.00652295856412,)oftype(2\"    \n",
       " \"1.00652295856412, 0)typeof(21.00652295856412,)conv\"    \n",
       " \"ert(Float64, 0)Float64(0,)sitofp(Float64, 0)<(21.0\"    \n",
       " \"0652295856412, 0.0)lt_float(21.00652295856412, 0.0\"    \n",
       " \")sqrt_llvm(21.00652295856412,)\"                        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod.(Xs[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys = (map(t->occursin(\"sqrt_llvm\", prod(t)), text));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 348)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Ys), length(Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19-element Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1}:\n",
       " [true true … false false; false false … false false; … ; false false … false false; false false … false false]  \n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; true false … false false; … ; false false … false false; false false … false false] "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs_vec = [[onehotbatch(x, alphabet) for x in Xs[i]] for i in 1:length(Xs)];\n",
    "Xs_vec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25-element Array{Flux.OneHotMatrix{Array{Flux.OneHotVector,1}},1}:\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [true false … false false; false true … true false; … ; false false … false false; false false … false false]   \n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … true false; … ; false false … false false; false false … false false] \n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ys = readdlm(\"y_results.csv\");\n",
    "labelset = unique(Ys)\n",
    "#dataset = [(onehotbatch(x, alphabet, '\\n'), onehot(Ys[i],labelset))\n",
    "#           for i in 1:length(Ys) for x in Xs[i]] |> shuffle\n",
    "dataset = shuffle([(Xs_vec[i], onehot(Ys[i], labelset)) for i in 1:length(Xs_vec)])\n",
    "Ys = last.(dataset)\n",
    "first.(dataset)[1][1:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348-element Array{Array{Bool,2},1}:\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " ⋮                                                                                                               \n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]\n",
       " [false false … false false; false false … false false; … ; false false … false false; false false … false false]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad sequences to equal lengths\n",
    "#Xs_padded = [hcat(x,repeat(stop,1,seq_len-size(x)[1])) for x in first.(dataset)]\n",
    "Xs_padded = [hcat(x[1:10]...) for x in first.(dataset)]\n",
    "#map(length, Xs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 972,290 items in our data. We use a train:test split of 90:10, stratified to ensure we have \n",
    "# the same share of \"bad\" and \"good\" traces in our train and test sets.\n",
    "\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = stratifiedobs((Xs_padded, Ys), p=0.9)\n",
    "\n",
    "train = [(Xtrain[i], Ytrain[i]) for i in 1:length(Ytrain)];\n",
    "test = [(Xtest[i], Ytest[i]) for i in 1:length(Ytest)];\n",
    "length(train), length(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58×500 Array{Bool,2}:\n",
       "  true   true  false  false  false  …  false  false  false  false  false\n",
       " false  false   true  false  false     false  false  false  false  false\n",
       " false  false  false   true  false     false  false  false  false  false\n",
       " false  false  false  false   true     false   true  false  false  false\n",
       " false  false  false  false  false     false  false   true  false  false\n",
       " false  false  false  false  false  …  false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false  …  false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false   true  false\n",
       "     ⋮                              ⋱      ⋮                            \n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false  …  false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false  …  false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false\n",
       " false  false  false  false  false     false  false  false  false  false"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.129354 -0.0241872 … -0.0101191 -0.217834; -0.205283 0.105929 … 0.151116 -0.221793; … ; -0.102469 -0.192997 … -0.0877722 -0.0654175; -0.22055 0.0625328 … -0.110478 0.115831] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked), Float32[-0.069226 0.0263257 … 0.12184 -0.0979717; 0.0420466 0.117881 … -0.144269 -0.0572768; … ; -0.0101115 -0.0503838 … 0.0683065 0.0389834; 0.0482987 -0.145225 … -0.0969547 0.139767] (tracked), Float32[-0.0433572 0.0325791 … -0.0879745 0.145104; -0.0882896 0.115542 … -0.0810297 0.125361; … ; -0.057541 0.0998544 … 0.139006 -0.0808067; 0.144499 -0.0519149 … 0.0960697 -0.142463] (tracked), Float32[0.0798743, 0.0759112, 0.151637, -0.0658528, -0.00590102, 0.144042, 0.0726334, -0.0810471, -0.13389, 0.126116  …  -0.0381014, -0.00151839, 0.0764952, -0.0388899, 0.0603149, -0.0646586, 0.0466692, -0.131691, 0.0934145, -0.0481369] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked), Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (tracked), Float32[0.169099 -0.145625 … -0.031572 0.18699; -0.110828 -0.114319 … -0.179888 0.249756] (tracked), Float32[0.0, 0.0] (tracked)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We set up our model architecture\n",
    "\n",
    "scanner = Chain(Dense(length(alphabet), seq_len, σ), LSTM(seq_len, seq_len))\n",
    "encoder = Dense(seq_len, 2)\n",
    "\n",
    "function model(x)\n",
    "  state = scanner.([x])[end]\n",
    "  Flux.reset!(scanner)\n",
    "  softmax(encoder(state))\n",
    "end\n",
    "\n",
    "loss(tup...) = begin\n",
    "    #@show typeof.(tup)\n",
    "    #@show size.(tup)\n",
    "    crossentropy(model(tup[1]), tup[2])\n",
    "end\n",
    "accuracy(tup...) = mean(argmax(model(tup[1])) .== argmax(tup[2]))\n",
    "\n",
    "opt = ADAM(0.000001)\n",
    "ps = params(scanner, encoder)\n",
    "#ps = params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#24 (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we set up our callbacks for reporting on training progress.\n",
    "mean(x) = sum(x)/length(x)\n",
    "#testacc() = mean(accuracy(t) for t in test)\n",
    "testloss() = mean(loss(t...) for t in test)\n",
    "\n",
    "evalcb = () -> @show testloss()#, testacc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(length(train), length(test)) = (313, 35)\n",
      "testloss() = 244.06038f0 (tracked)\n",
      "testloss() = 242.83792f0 (tracked)\n",
      "testloss() = 241.65282f0 (tracked)\n",
      "testloss() = 240.50385f0 (tracked)\n",
      "testloss() = 239.39015f0 (tracked)\n",
      "testloss() = 238.31078f0 (tracked)\n",
      "testloss() = 237.26488f0 (tracked)\n",
      "testloss() = 236.25148f0 (tracked)\n",
      "testloss() = 235.26956f0 (tracked)\n",
      "testloss() = 234.31836f0 (tracked)\n",
      "testloss() = 233.39685f0 (tracked)\n",
      "testloss() = 232.50427f0 (tracked)\n",
      "testloss() = 231.63972f0 (tracked)\n",
      "testloss() = 230.80249f0 (tracked)\n",
      "testloss() = 229.99174f0 (tracked)\n",
      "testloss() = 229.20673f0 (tracked)\n",
      "testloss() = 228.44658f0 (tracked)\n",
      "testloss() = 227.71059f0 (tracked)\n",
      "testloss() = 226.998f0 (tracked)\n",
      "testloss() = 226.3081f0 (tracked)\n",
      "testloss() = 225.64024f0 (tracked)\n",
      "testloss() = 224.99364f0 (tracked)\n",
      "testloss() = 224.36768f0 (tracked)\n",
      "testloss() = 223.7616f0 (tracked)\n",
      "testloss() = 223.1748f0 (tracked)\n",
      "testloss() = 222.60672f0 (tracked)\n",
      "testloss() = 222.0568f0 (tracked)\n",
      "testloss() = 221.52449f0 (tracked)\n",
      "testloss() = 221.00935f0 (tracked)\n",
      "testloss() = 220.51064f0 (tracked)\n",
      "testloss() = 220.02776f0 (tracked)\n",
      "testloss() = 219.56055f0 (tracked)\n",
      "testloss() = 219.1082f0 (tracked)\n",
      "testloss() = 218.67033f0 (tracked)\n",
      "testloss() = 218.24661f0 (tracked)\n",
      "testloss() = 217.83652f0 (tracked)\n",
      "testloss() = 217.43953f0 (tracked)\n",
      "testloss() = 217.05533f0 (tracked)\n",
      "testloss() = 216.68356f0 (tracked)\n",
      "testloss() = 216.32375f0 (tracked)\n",
      "testloss() = 215.9755f0 (tracked)\n",
      "testloss() = 215.63863f0 (tracked)\n",
      "testloss() = 215.3126f0 (tracked)\n",
      "testloss() = 214.99713f0 (tracked)\n",
      "testloss() = 214.69183f0 (tracked)\n",
      "testloss() = 214.39641f0 (tracked)\n",
      "testloss() = 214.1105f0 (tracked)\n",
      "testloss() = 213.8338f0 (tracked)\n",
      "testloss() = 213.56604f0 (tracked)\n",
      "testloss() = 213.30695f0 (tracked)\n"
     ]
    }
   ],
   "source": [
    "# Now, train!\n",
    "@show length(train), length(test)\n",
    "epochs = 50\n",
    "for e in 1:epochs\n",
    "    Flux.train!(loss, ps, train, opt, cb = throttle(evalcb, 10))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[-0.139871 -0.0347597 … -0.0213785 -0.217834; -0.190121 0.121064 … 0.16649 -0.221793; … ; -0.107366 -0.197782 … -0.0929333 -0.0654175; -0.222207 0.0593458 … -0.111654 0.115831] (tracked), Float32[-0.0104232, 0.0150795, 0.0155962, -0.00930544, 0.00410825, -0.0139495, 0.0148873, 0.014865, 0.000389495, 0.00345307  …  0.00894054, 0.0150887, -0.00572559, -0.0140178, 0.0148194, 0.0153684, -0.0142857, -0.00572933, -0.00536107, -0.00245235] (tracked), Float32[-0.0690327 0.0264817 … 0.121986 -0.0978056; 0.0498987 0.125716 … -0.136427 -0.0494317; … ; 0.00559051 -0.0346823 … 0.0839965 0.0546742; 0.0640606 -0.129461 … -0.0811916 0.15553] (tracked), Float32[-0.0417015 0.0342454 … -0.0896309 0.143439; -0.0951674 0.108681 … -0.0741613 0.132219; … ; -0.0751735 0.08222 … 0.156634 -0.0631734; 0.126793 -0.0696225 … 0.113772 -0.124756] (tracked), Float32[0.0800481, 0.0837521, 0.148407, -0.0790332, 0.0105961, 0.130021, 0.0630973, -0.0658107, -0.135941, 0.141698  …  -0.0224647, 0.0133192, 0.0912948, -0.0380693, 0.0755476, -0.0508781, 0.0325882, -0.116653, 0.109113, -0.0323714] (tracked), Float32[-0.0148584, -0.0150116, 0.0148392, 0.0150744, -0.0153006, -0.0151183, -0.0150436, -0.0151836, -0.015303, -0.0149209  …  0.0152811, 0.0149321, -0.0150606, -0.0149256, 0.0148433, -0.0147554, 0.0149571, -0.01498, 0.014768, 0.0150553] (tracked), Float32[-0.0145493, 0.0147424, 0.0145161, 0.014099, 0.0146985, -0.0144519, -0.0144574, -0.0148868, 0.0144076, -0.0147348  …  -0.0147498, -0.0146276, -0.0146021, -0.014528, 0.0146386, -0.0147127, -0.0144808, 0.0146392, -0.0146997, 0.014808] (tracked), Float32[0.170056 -0.153686 … -0.0158306 0.171296; -0.111787 -0.106257 … -0.195631 0.265452] (tracked), Float32[-0.014513, 0.0145131] (tracked)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.(Xs_padded)[:,end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t0.269\t1.5504481f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.266\t0.23609789f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "1\t0.269\t1.5504481f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.266\t0.23609789f0 (tracked)\n",
      "1\t0.274\t1.537193f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "1\t0.274\t1.537193f0 (tracked)\n",
      "2\t0.27\t0.23873302f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.264\t1.5668414f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.27\t0.23908937f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "1\t0.274\t1.537193f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.272\t1.5417317f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "1\t0.264\t1.5653542f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "1\t0.273\t1.5406557f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "1\t0.272\t1.5424143f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "1\t0.276\t1.5313448f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "1\t0.272\t1.5424143f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.265\t1.5631661f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "1\t0.269\t1.5504481f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "1\t0.272\t1.5424143f0 (tracked)\n",
      "1\t0.264\t1.5668414f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "1\t0.269\t1.5504481f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "1\t0.274\t1.537193f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "1\t0.272\t1.5424143f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.266\t0.23609789f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "1\t0.272\t1.5417317f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.273\t0.2411045f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.271\t0.23955657f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "1\t0.265\t1.5631661f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "1\t0.269\t1.5504481f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "1\t0.275\t1.535116f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "1\t0.272\t1.5424143f0 (tracked)\n",
      "2\t0.271\t0.23978798f0 (tracked)\n",
      "1\t0.272\t1.5417317f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "1\t0.274\t1.537193f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.271\t1.545773f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.264\t0.23447493f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.271\t0.23971389f0 (tracked)\n",
      "2\t0.269\t0.23845132f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24081151f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24033804f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "1\t0.264\t1.5653542f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.273\t0.24143757f0 (tracked)\n",
      "1\t0.266\t1.5592362f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "2\t0.272\t0.24062566f0 (tracked)\n",
      "1\t0.272\t1.5434723f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.264\t0.23408234f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.274\t0.2420508f0 (tracked)\n",
      "2\t0.275\t0.24262035f0 (tracked)\n",
      "2\t0.276\t0.24365835f0 (tracked)\n",
      "2\t0.265\t0.2350539f0 (tracked)\n",
      "1\t0.272\t1.5417317f0 (tracked)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "300-element Array{Any,1}:\n",
       " (1, 0.264f0, 1.5653542f0 (tracked)) \n",
       " (1, 0.264f0, 1.5653542f0 (tracked)) \n",
       " (1, 0.264f0, 1.5668414f0 (tracked)) \n",
       " (1, 0.264f0, 1.5668414f0 (tracked)) \n",
       " (1, 0.265f0, 1.5631661f0 (tracked)) \n",
       " (1, 0.265f0, 1.5631661f0 (tracked)) \n",
       " (1, 0.266f0, 1.5592362f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.271f0, 1.545773f0 (tracked))  \n",
       " ⋮                                   \n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))\n",
       " (2, 0.276f0, 0.24365835f0 (tracked))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumη = [0.0,0.0]\n",
    "df = []\n",
    "for i in 1:300\n",
    "    yhat = model(train[i][1])[:,end]\n",
    "    η = crossentropy(yhat, train[i][2])\n",
    "    sumη += η.*train[i][2]\n",
    "    υ = Flux.onecold(train[i][2])\n",
    "    logods = round(Flux.data(yhat[1]/yhat[2]);digits=3)\n",
    "    push!(df, (υ, logods, η))\n",
    "    #println(\"$υ\\t$logods\\t$η\")\n",
    "end\n",
    "ηbar = sumη./sum(train[i][2] for i in 1:length(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50-element Array{Any,1}:\n",
       " (1, 0.264f0, 1.5653542f0 (tracked)) \n",
       " (1, 0.264f0, 1.5653542f0 (tracked)) \n",
       " (1, 0.264f0, 1.5668414f0 (tracked)) \n",
       " (1, 0.264f0, 1.5668414f0 (tracked)) \n",
       " (1, 0.265f0, 1.5631661f0 (tracked)) \n",
       " (1, 0.265f0, 1.5631661f0 (tracked)) \n",
       " (1, 0.266f0, 1.5592362f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.269f0, 1.5504481f0 (tracked)) \n",
       " (1, 0.271f0, 1.545773f0 (tracked))  \n",
       " ⋮                                   \n",
       " (1, 0.275f0, 1.535116f0 (tracked))  \n",
       " (1, 0.275f0, 1.535116f0 (tracked))  \n",
       " (1, 0.275f0, 1.535116f0 (tracked))  \n",
       " (1, 0.275f0, 1.535116f0 (tracked))  \n",
       " (1, 0.275f0, 1.535116f0 (tracked))  \n",
       " (1, 0.276f0, 1.5313448f0 (tracked)) \n",
       " (2, 0.264f0, 0.23408234f0 (tracked))\n",
       " (2, 0.264f0, 0.23408234f0 (tracked))\n",
       " (2, 0.264f0, 0.23408234f0 (tracked))\n",
       " (2, 0.264f0, 0.23408234f0 (tracked))\n",
       " (2, 0.264f0, 0.23408234f0 (tracked))\n",
       " (2, 0.264f0, 0.23408234f0 (tracked))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000-element Array{Tuple{},1}:\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ⋮ \n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()\n",
       " ()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(x->size(x[end]), Xs_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/dev/SemanticModels/Project.toml`\n",
      " \u001b[90m [be33ccc6]\u001b[39m\u001b[92m + CUDAnative v1.0.1\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/dev/SemanticModels/Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "Pkg.add([\"CuArrays\",\"CUDAnative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m LLVM ──────→ `~/.julia/packages/LLVM/tPWXv/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m CUDAdrv ───→ `~/.julia/packages/CUDAdrv/JWljj/deps/build.log`\n",
      "\u001b[32m\u001b[1m  Building\u001b[22m\u001b[39m CUDAnative → `~/.julia/packages/CUDAnative/Mdd3w/deps/build.log`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Error: Error building `CUDAnative`: \n",
      "│ ERROR: LoadError: CUDA toolkit at  doesn't contain nvcc\n",
      "│ Stacktrace:\n",
      "│  [1] error(::String) at ./error.jl:33\n",
      "│  [2] find_toolkit_version(::Array{String,1}) at /home/jfairbanks6/.julia/packages/CUDAapi/AVyQs/src/discovery.jl:277\n",
      "│  [3] main() at /home/jfairbanks6/.julia/packages/CUDAnative/Mdd3w/deps/build.jl:125\n",
      "│  [4] top-level scope at none:0\n",
      "│  [5] include at ./boot.jl:317 [inlined]\n",
      "│  [6] include_relative(::Module, ::String) at ./loading.jl:1044\n",
      "│  [7] include(::Module, ::String) at ./sysimg.jl:29\n",
      "│  [8] include(::String) at ./client.jl:392\n",
      "│  [9] top-level scope at none:0\n",
      "│ in expression starting at /home/jfairbanks6/.julia/packages/CUDAnative/Mdd3w/deps/build.jl:165\n",
      "└ @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.0/Pkg/src/Operations.jl:1097\n"
     ]
    }
   ],
   "source": [
    "Pkg.build(\"CUDAnative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58-element Array{Bool,1}:\n",
       " false\n",
       "  true\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       "     ⋮\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false\n",
       " false"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[1][1][:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
